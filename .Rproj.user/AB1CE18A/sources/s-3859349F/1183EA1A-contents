---
title: "REISA Data Analytics"
author: "Aurecon, by Paul Nel"
date: "11 October 2019"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
editor_options: 
  
  chunk_output_type: console
---

```{r setup, include=FALSE}
  knitr::opts_chunk$set(include = TRUE) 
  knitr::opts_chunk$set(fig.height = 6, fig.width = 8, out.width = '50%', fig.align = "center")
  options(width = 90)
  options(scipen=999)
```

## Undersanding the Data

This presents a first pass analysis of the irradiation data with the main aim to start understanding each of the feautres.  It is clear that more work is required regarding understanding the correlation between certain features but also that the there is much work ito of getting the correct data (qaulity and quantity) in play.

## Reading data

Read data from temporary csv file created by the python script `pyr_features.py`.  This data should read directly from Mongo in future.  The building names are converted to factors to allow them to be analysed in a regression model.

Some of the features are removed before analysis.  I have also removed all the samples where `pyr_hor >2000` and `pyr_inc > 2000`.

```{r}

all_data = read.csv("pyranometer_hourly_kathu.csv")
data = all_data[,c(-1, -2, -3,-8)]
data = na.omit(data)
data =data[data$pyr_hor<2000,]
data =data[data$pyr_inc<2000,]
data[,"building"] = as.factor(data[,"building"])
attach(data)
names(data)
```

This produces a data set without any `Nan`'s and with `` `r length(names(data)) ` `` features and `` `r nrow(data)` `` samples.

## Linear Regression

Firstly we fit a normal linear regression model without any regularisation.  There are few enough dimensions to allow us to evaluate the significance manually.

### Fitting the model

```{r}
lm.fit1 = lm(pyr_hor ~  day + hour + mod_temp + pyr_inc, data=data)
summary(lm.fit1)
cooks = cooks.distance(lm.fit1)
outliers = which(cooks>0.05, cooks)
plot(lm.fit1, which=c(4,5))
```

From Cook's distance it is clear that sample 51617 is an outlier.  We will remove this sample and re-train.

```{r}
data = data[-outliers,]
lm.fit2 = lm(pyr_hor ~  day + hour + mod_temp + pyr_inc, data=data)
summary(lm.fit2)
```

The analysis suggests that all four features are highly significant (relatively high t-values) in estimating the reading for horizontal irradiation.  Also the Adjusted R-Squared of 0.914 is quite high and may suggest overfitting since all the data is used for training. Hence I will use a 70-30 split for training-test data to verify confidence on future test samples.

```{r}
set.seed(1)
n = dim(data)[1]
test.id = sample(seq(1,n),round(n*0.3,0))
```

Now fit the model on the training sample.

```{r}
lm.train = lm(pyr_hor ~  day + hour + mod_temp + pyr_inc
              , data=data[-test.id,])
```
and use this model to predict on the test data:
```{r}
test.pred = predict(lm.train,newdata = data[test.id,])
test.y = data[test.id,"pyr_hor"]
```

To verify the fit on the test data, we calcualte the adjusted R-Squared for the test data:
```{r}
SS.total =sum((test.y - mean(test.y))^2)
SS.residual = sum((test.y - test.pred)^2)
SS.regression = sum((test.pred - mean(test.y))^2)
test.rsq = 1 - SS.residual/SS.total  
adj.Rs = SS.regression/SS.total 
```

which turns out to be similar to that of the training set at `` `r round(adj.Rs,3)` ``.

### Finding optimal subsets of features

To see if there are any subset of variabels that is optimal we will use `R`'s `regsubsets`
```{r message=FALSE, warning=FALSE}
require(leaps)
p = 4
msize = 1:p
subset = regsubsets(pyr_hor ~  day + hour + mod_temp + pyr_inc,data=data[-test.id,], nvmax = p, method="exhaustive")
best.subset = summary(subset)
best.model = which.min(best.subset$cp)
```
This confirms that we get the best performance on the training data when including `` `r which.min(best.subset$cp)` `` variables in the regression model.

### Discussion
This would suggest that we can use this model to predict the value of `pyr_hor.`

However if we look at some sample test data  below:

```{r}
example.id = sample(test.id,20)
all_data[example.id,]
pred.example = predict(lm.train,newdata = data[example.id,])
```
Which gives us the following comaprison:

```{r}
cbind(data[example.id,'pyr_hor'],pred.example)
plot(-20:1200,-20:1200, type='n', xlab='actual', ylab='predicted', main='Samle predictions for pyr_hor')
points(data[example.id,'pyr_hor'], pred.example, pch=19,col='blue')
lines(x = c(-100,1250), y = c(-100,1250), lty=2, col ='red')
```
The variance seems to high.  This is probably due to the non-linearity of the the irradiation and hence we need to do some further analysis.
